"""PDF ç´¢å¼•æ„å»ºå™¨.

å°† DND è§„åˆ™ä¹¦ PDF è§£æã€åˆ†å—ã€å‘é‡åŒ–ï¼Œå­˜å…¥ ChromaDBã€‚
"""

import logging
import os
import re
from pathlib import Path
from typing import Optional

from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from openai import OpenAI

from src.rag.config import (
    CHUNK_OVERLAP,
    CHUNK_SIZE,
    CHROMA_PERSIST_DIR,
    COLLECTION_NAME,
    EMBEDDING_DIMENSIONS,
    EMBEDDING_PROVIDER,
    OPENAI_EMBEDDING_MODEL,
    PDF_PATH,
    SILICONFLOW_API_BASE,
    SILICONFLOW_EMBEDDING_MODEL,
)

logger = logging.getLogger(__name__)
from dotenv import load_dotenv
load_dotenv()


class DashScopeEmbeddings(Embeddings):
    """DashScope Embeddings åŒ…è£…ç±»ï¼Œæ”¯æŒ text-embedding-v4 å’Œ dimensions å‚æ•°.
    
    ä½¿ç”¨ OpenAI å…¼å®¹å®¢æˆ·ç«¯è°ƒç”¨ DashScope APIã€‚
    """
    
    def __init__(
        self,
        api_key: str,
        base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
        model: str = "text-embedding-v4",
        dimensions: Optional[int] = None
    ):
        """åˆå§‹åŒ– DashScope Embeddings.
        
        Args:
            api_key: DashScope API Key
            base_url: API åŸºç¡€ URL
            model: æ¨¡å‹åç§°
            dimensions: å‘é‡ç»´åº¦ï¼ˆä»… text-embedding-v4 æ”¯æŒï¼‰
        """
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )
        self.model = model
        self.dimensions = dimensions
    
    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        """æ‰¹é‡åµŒå…¥æ–‡æ¡£.
        
        DashScope API é™åˆ¶æ¯æ‰¹æœ€å¤š 10 ä¸ªæ–‡æœ¬ï¼Œéœ€è¦åˆ†æ‰¹å¤„ç†ã€‚
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        # DashScope API é™åˆ¶ï¼šæ¯æ‰¹æœ€å¤š 10 ä¸ªæ–‡æœ¬
        BATCH_SIZE = 10
        all_embeddings = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), BATCH_SIZE):
            batch_texts = texts[i:i + BATCH_SIZE]
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                "model": self.model,
                "input": batch_texts,
            }
            # å¦‚æœæŒ‡å®šäº† dimensions ä¸”æ¨¡å‹æ”¯æŒï¼Œåˆ™æ·»åŠ è¯¥å‚æ•°
            if self.dimensions is not None and self.model == "text-embedding-v4":
                params["dimensions"] = self.dimensions
            
            try:
                resp = self.client.embeddings.create(**params)
                batch_embeddings = [item.embedding for item in resp.data]
                all_embeddings.extend(batch_embeddings)
                
                # è®°å½•è¿›åº¦ï¼ˆæ¯ 50 ä¸ªæ–‡æœ¬è®°å½•ä¸€æ¬¡ï¼‰
                processed = min(i + BATCH_SIZE, len(texts))
                if processed % 50 == 0 or processed >= len(texts):
                    logger.info(f"ğŸ“Š å‘é‡åŒ–è¿›åº¦: {processed}/{len(texts)} ({processed*100//len(texts)}%)")
            except Exception as e:
                logger.error(f"DashScope Embedding è°ƒç”¨å¤±è´¥ (æ‰¹æ¬¡ {i//BATCH_SIZE + 1}): {e}")
                raise
        
        return all_embeddings
    
    def embed_query(self, text: str) -> list[float]:
        """åµŒå…¥å•ä¸ªæŸ¥è¯¢æ–‡æœ¬.
        
        Args:
            text: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            å‘é‡
        """
        return self.embed_documents([text])[0]


def get_embeddings():
    """æ ¹æ®é…ç½®è·å– Embedding æ¨¡å‹.
    
    æ”¯æŒ:
    - openai: OpenAI text-embedding-3-small
    - siliconflow: ç¡…åŸºæµåŠ¨ BGE æ¨¡å‹ï¼ˆå›½å†…æ¨èï¼‰
    - dashscope: é€šä¹‰åƒé—® Embedding æ¨¡å‹ï¼ˆé€šè¿‡å…¼å®¹æ¨¡å¼ï¼Œæ”¯æŒ text-embedding-v4 å’Œ dimensionsï¼‰
    """
    from langchain_openai import OpenAIEmbeddings
    
    if EMBEDDING_PROVIDER == "siliconflow":
        return OpenAIEmbeddings(
            model=SILICONFLOW_EMBEDDING_MODEL,
            openai_api_key=os.getenv("SILICONFLOW_API_KEY"),
            openai_api_base=SILICONFLOW_API_BASE,
        )
    else:
        api_base = os.getenv("OPENAI_API_BASE")
        api_key = os.getenv("OPENAI_API_KEY") or os.getenv("DASHSCOPE_API_KEY")
        model = OPENAI_EMBEDDING_MODEL
        
        # å¦‚æœä½¿ç”¨ DashScope å…¼å®¹æ¨¡å¼ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„ DashScopeEmbeddings
        if api_base and "dashscope" in api_base.lower():
            # ä½¿ç”¨ DashScope è‡ªå®šä¹‰ Embeddings ç±»ï¼Œæ”¯æŒ text-embedding-v4 å’Œ dimensions
            logger.info(f"ä½¿ç”¨ DashScope Embeddings: model={model}, dimensions={EMBEDDING_DIMENSIONS}")
            return DashScopeEmbeddings(
                api_key=api_key,
                base_url=api_base,
                model=model,
                dimensions=EMBEDDING_DIMENSIONS if model == "text-embedding-v4" else None
            )
        
        # å…¶ä»–æƒ…å†µä½¿ç”¨æ ‡å‡†çš„ OpenAIEmbeddings
        return OpenAIEmbeddings(
            model=model,
            openai_api_base=api_base,
            openai_api_key=api_key
        )


def load_pdf(pdf_path: Path) -> list[Document]:
    """åŠ è½½ PDF æ–‡ä»¶.
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        
    Returns:
        Document åˆ—è¡¨ï¼Œæ¯é¡µä¸€ä¸ª Document
    """
    from langchain_community.document_loaders import PyPDFLoader

    logger.info(f"ğŸ“– æ­£åœ¨åŠ è½½ PDF: {pdf_path}")

    if not pdf_path.exists():
        raise FileNotFoundError(f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")

    loader = PyPDFLoader(str(pdf_path))
    documents = loader.load()

    logger.info(f"âœ… åŠ è½½å®Œæˆ: {len(documents)} é¡µ")
    return documents


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤å™ªéŸ³.
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        æ¸…ç†åçš„æ–‡æœ¬
    """
    # å»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    # å»é™¤é¡µçœ‰é¡µè„šå¸¸è§æ¨¡å¼ï¼ˆå¯æ ¹æ®å®é™… PDF è°ƒæ•´ï¼‰
    text = re.sub(r'ç¬¬\s*\d+\s*é¡µ', '', text)
    text = re.sub(r'Page\s*\d+', '', text, flags=re.IGNORECASE)
    # å»é™¤é¦–å°¾ç©ºç™½
    text = text.strip()
    return text


def preprocess_documents(documents: list[Document]) -> list[Document]:
    """é¢„å¤„ç†æ–‡æ¡£ï¼Œæ¸…ç†æ–‡æœ¬å¹¶æ·»åŠ å…ƒæ•°æ®.
    
    Args:
        documents: åŸå§‹ Document åˆ—è¡¨
        
    Returns:
        é¢„å¤„ç†åçš„ Document åˆ—è¡¨
    """
    processed = []
    for doc in documents:
        cleaned_content = clean_text(doc.page_content)
        if len(cleaned_content) < 50:  # è·³è¿‡å¤ªçŸ­çš„é¡µé¢
            continue

        # ä¿ç•™å¹¶å¢å¼ºå…ƒæ•°æ®
        metadata = doc.metadata.copy()
        metadata["source_type"] = "dnd_phb"
        metadata["language"] = "zh"

        processed.append(Document(
            page_content=cleaned_content,
            metadata=metadata
        ))

    logger.info(f"ğŸ“ é¢„å¤„ç†å®Œæˆ: {len(processed)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
    return processed


def split_documents(documents: list[Document]) -> list[Document]:
    """å°†æ–‡æ¡£åˆ†å—.
    
    ä½¿ç”¨é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–çš„åˆ†éš”ç¬¦ã€‚
    
    Args:
        documents: Document åˆ—è¡¨
        
    Returns:
        åˆ†å—åçš„ Document åˆ—è¡¨
    """
    # ä¸­æ–‡å‹å¥½çš„åˆ†éš”ç¬¦ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    separators = [
        "\n\n",  # æ®µè½
        "\n",  # æ¢è¡Œ
        "ã€‚",  # å¥å·
        "ï¼",  # æ„Ÿå¹å·
        "ï¼Ÿ",  # é—®å·
        "ï¼›",  # åˆ†å·
        "ï¼Œ",  # é€—å·
        " ",  # ç©ºæ ¼
        ""  # å­—ç¬¦çº§åˆ«ï¼ˆæœ€åæ‰‹æ®µï¼‰
    ]

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=separators,
        length_function=len,
    )

    chunks = splitter.split_documents(documents)
    
    # è¿‡æ»¤æ— æ•ˆçš„æ–‡æ¡£å—ï¼šç¡®ä¿å†…å®¹æ˜¯éç©ºå­—ç¬¦ä¸²
    valid_chunks = []
    for chunk in chunks:
        content = chunk.page_content
        # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²ç±»å‹ä¸”éç©º
        if isinstance(content, str) and content.strip():
            valid_chunks.append(chunk)
        else:
            logger.warning(f"è·³è¿‡æ— æ•ˆæ–‡æ¡£å—: ç±»å‹={type(content)}, é•¿åº¦={len(content) if content else 0}")
    
    logger.info(f"âœ‚ï¸ åˆ†å—å®Œæˆ: {len(valid_chunks)} ä¸ªæœ‰æ•ˆæ–‡æ¡£å— (è¿‡æ»¤äº† {len(chunks) - len(valid_chunks)} ä¸ªæ— æ•ˆå—)")
    return valid_chunks


def build_index(
        pdf_path: Optional[Path] = None,
        persist_directory: Optional[Path] = None,
        force_rebuild: bool = False
) -> "Chroma":
    """æ„å»ºå‘é‡ç´¢å¼•.
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
        persist_directory: æŒä¹…åŒ–ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
        
    Returns:
        ChromaDB å‘é‡æ•°æ®åº“å®ä¾‹
    """
    from langchain_chroma import Chroma

    pdf_path = pdf_path or PDF_PATH
    persist_directory = persist_directory or CHROMA_PERSIST_DIR

    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç´¢å¼•
    if persist_directory.exists() and not force_rebuild:
        logger.info(f"ğŸ“‚ å‘ç°å·²æœ‰ç´¢å¼•: {persist_directory}")
        logger.info("å¦‚éœ€é‡å»ºï¼Œè¯·ä½¿ç”¨ force_rebuild=True")
        return Chroma(
            persist_directory=str(persist_directory),
            embedding_function=get_embeddings(),
            collection_name=COLLECTION_NAME
        )

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    persist_directory.mkdir(parents=True, exist_ok=True)

    logger.info("ğŸš€ å¼€å§‹æ„å»ºç´¢å¼•...")

    # 1. åŠ è½½ PDF
    documents = load_pdf(pdf_path)

    # 2. é¢„å¤„ç†
    processed_docs = preprocess_documents(documents)

    # 3. åˆ†å—
    chunks = split_documents(processed_docs)

    # 4. å†æ¬¡éªŒè¯æ–‡æ¡£å—çš„æœ‰æ•ˆæ€§ï¼ˆåŒé‡ä¿é™©ï¼‰
    valid_chunks = []
    for chunk in chunks:
        if isinstance(chunk.page_content, str) and chunk.page_content.strip():
            valid_chunks.append(chunk)
    
    if len(valid_chunks) < len(chunks):
        logger.warning(f"è¿‡æ»¤äº† {len(chunks) - len(valid_chunks)} ä¸ªæ— æ•ˆæ–‡æ¡£å—")
        chunks = valid_chunks
    
    if not chunks:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å—å¯ä»¥å‘é‡åŒ–ï¼è¯·æ£€æŸ¥ PDF å†…å®¹å’Œåˆ†å—é…ç½®ã€‚")
    
    # 5. è·å– Embedding æ¨¡å‹
    embeddings = get_embeddings()

    # 6. åˆ›å»ºå‘é‡æ•°æ®åº“
    logger.info("ğŸ”„ æ­£åœ¨å‘é‡åŒ–å¹¶å­˜å‚¨ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
    logger.info(f"   å°†å¤„ç† {len(chunks)} ä¸ªæ–‡æ¡£å—...")
    
    # éªŒè¯ embedding æ¨¡å‹é…ç½®
    api_base = os.getenv("OPENAI_API_BASE", "")
    if api_base and "dashscope" in api_base.lower():
        logger.info(f"âœ… ä½¿ç”¨ DashScope Embeddings (model={OPENAI_EMBEDDING_MODEL}, dimensions={EMBEDDING_DIMENSIONS})")

    try:
        vectordb = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=str(persist_directory),
            collection_name=COLLECTION_NAME
        )
    except Exception as e:
        error_msg = str(e)
        if "contents is neither str nor list of str" in error_msg or "InvalidParameter" in error_msg:
            logger.error("âŒ Embedding API å‚æ•°æ ¼å¼é”™è¯¯ï¼")
            logger.error("   å¯èƒ½çš„åŸå› ï¼š")
            logger.error("   1. API Key ä¸æ­£ç¡®æˆ–æœªè®¾ç½®")
            logger.error("   2. æ¨¡å‹åç§°ä¸æ­£ç¡®")
            logger.error("   3. dimensions å‚æ•°å€¼ä¸æ­£ç¡®ï¼ˆåº”ä¸º 256, 512, æˆ– 1024ï¼‰")
            logger.error("   å»ºè®®è§£å†³æ–¹æ¡ˆï¼š")
            logger.error("   - æ£€æŸ¥ DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
            logger.error("   - ç¡®è®¤æ¨¡å‹åç§°æ­£ç¡®ï¼ˆtext-embedding-v4ï¼‰")
            logger.error("   - æ£€æŸ¥ RAG_EMBEDDING_DIMENSIONS ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰å€¼ï¼š256, 512, 1024ï¼‰")
        raise

    logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ!")
    logger.info(f"   - æ–‡æ¡£å—æ•°é‡: {len(chunks)}")
    logger.info(f"   - å­˜å‚¨ä½ç½®: {persist_directory}")

    return vectordb


def get_index_stats(persist_directory: Optional[Path] = None) -> dict:
    """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯.
    
    Args:
        persist_directory: æŒä¹…åŒ–ç›®å½•
        
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    from langchain_chroma import Chroma

    persist_directory = persist_directory or CHROMA_PERSIST_DIR

    if not persist_directory.exists():
        return {"exists": False, "error": "ç´¢å¼•ä¸å­˜åœ¨"}

    try:
        vectordb = Chroma(
            persist_directory=str(persist_directory),
            embedding_function=get_embeddings(),
            collection_name=COLLECTION_NAME
        )

        # è·å–é›†åˆä¿¡æ¯
        collection = vectordb._collection
        count = collection.count()

        return {
            "exists": True,
            "document_count": count,
            "collection_name": COLLECTION_NAME,
            "persist_directory": str(persist_directory)
        }
    except Exception as e:
        return {"exists": False, "error": str(e)}


# ============================================================
# CLI å…¥å£
# ============================================================

def main():
    """å‘½ä»¤è¡Œå…¥å£."""
    import argparse

    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s"
    )

    parser = argparse.ArgumentParser(
        description="DND è§„åˆ™ä¹¦ PDF ç´¢å¼•æ„å»ºå·¥å…·"
    )
    parser.add_argument(
        "--pdf",
        type=str,
        default=str(PDF_PATH),
        help=f"PDF æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {PDF_PATH})"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=str(CHROMA_PERSIST_DIR),
        help=f"ç´¢å¼•è¾“å‡ºç›®å½• (é»˜è®¤: {CHROMA_PERSIST_DIR})"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡å»ºç´¢å¼•ï¼ˆè¦†ç›–å·²æœ‰ç´¢å¼•ï¼‰"
    )
    parser.add_argument(
        "--stats",
        action="store_true",
        help="ä»…æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"
    )

    args = parser.parse_args()

    if args.stats:
        stats = get_index_stats(Path(args.output))
        print("\nğŸ“Š ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            print(f"   {key}: {value}")
        return

    # æ„å»ºç´¢å¼•
    build_index(
        pdf_path=Path(args.pdf),
        persist_directory=Path(args.output),
        force_rebuild=args.force
    )


if __name__ == "__main__":
    main()
