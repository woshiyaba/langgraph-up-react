"""PDF ç´¢å¼•æ„å»ºå™¨.

å°† DND è§„åˆ™ä¹¦ PDF è§£æã€åˆ†å—ã€å‘é‡åŒ–ï¼Œå­˜å…¥ ChromaDBã€‚
"""

import logging
import os
import re
from pathlib import Path
from typing import Optional

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

from src.rag.config import (
    CHUNK_OVERLAP,
    CHUNK_SIZE,
    CHROMA_PERSIST_DIR,
    COLLECTION_NAME,
    EMBEDDING_PROVIDER,
    OPENAI_EMBEDDING_MODEL,
    PDF_PATH,
    SILICONFLOW_API_BASE,
    SILICONFLOW_EMBEDDING_MODEL,
)

logger = logging.getLogger(__name__)


def get_embeddings():
    """æ ¹æ®é…ç½®è·å– Embedding æ¨¡å‹.
    
    æ”¯æŒ:
    - openai: OpenAI text-embedding-3-small
    - siliconflow: ç¡…åŸºæµåŠ¨ BGE æ¨¡å‹ï¼ˆå›½å†…æ¨èï¼‰
    """
    if EMBEDDING_PROVIDER == "siliconflow":
        from langchain_openai import OpenAIEmbeddings
        return OpenAIEmbeddings(
            model=SILICONFLOW_EMBEDDING_MODEL,
            openai_api_key=os.getenv("SILICONFLOW_API_KEY"),
            openai_api_base=SILICONFLOW_API_BASE,
        )
    else:
        # é»˜è®¤ä½¿ç”¨ OpenAI
        from langchain_openai import OpenAIEmbeddings
        return OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL)


def load_pdf(pdf_path: Path) -> list[Document]:
    """åŠ è½½ PDF æ–‡ä»¶.
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        
    Returns:
        Document åˆ—è¡¨ï¼Œæ¯é¡µä¸€ä¸ª Document
    """
    from langchain_community.document_loaders import PyPDFLoader
    
    logger.info(f"ğŸ“– æ­£åœ¨åŠ è½½ PDF: {pdf_path}")
    
    if not pdf_path.exists():
        raise FileNotFoundError(f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
    
    loader = PyPDFLoader(str(pdf_path))
    documents = loader.load()
    
    logger.info(f"âœ… åŠ è½½å®Œæˆ: {len(documents)} é¡µ")
    return documents


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤å™ªéŸ³.
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        æ¸…ç†åçš„æ–‡æœ¬
    """
    # å»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    # å»é™¤é¡µçœ‰é¡µè„šå¸¸è§æ¨¡å¼ï¼ˆå¯æ ¹æ®å®é™… PDF è°ƒæ•´ï¼‰
    text = re.sub(r'ç¬¬\s*\d+\s*é¡µ', '', text)
    text = re.sub(r'Page\s*\d+', '', text, flags=re.IGNORECASE)
    # å»é™¤é¦–å°¾ç©ºç™½
    text = text.strip()
    return text


def preprocess_documents(documents: list[Document]) -> list[Document]:
    """é¢„å¤„ç†æ–‡æ¡£ï¼Œæ¸…ç†æ–‡æœ¬å¹¶æ·»åŠ å…ƒæ•°æ®.
    
    Args:
        documents: åŸå§‹ Document åˆ—è¡¨
        
    Returns:
        é¢„å¤„ç†åçš„ Document åˆ—è¡¨
    """
    processed = []
    for doc in documents:
        cleaned_content = clean_text(doc.page_content)
        if len(cleaned_content) < 50:  # è·³è¿‡å¤ªçŸ­çš„é¡µé¢
            continue
        
        # ä¿ç•™å¹¶å¢å¼ºå…ƒæ•°æ®
        metadata = doc.metadata.copy()
        metadata["source_type"] = "dnd_phb"
        metadata["language"] = "zh"
        
        processed.append(Document(
            page_content=cleaned_content,
            metadata=metadata
        ))
    
    logger.info(f"ğŸ“ é¢„å¤„ç†å®Œæˆ: {len(processed)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
    return processed


def split_documents(documents: list[Document]) -> list[Document]:
    """å°†æ–‡æ¡£åˆ†å—.
    
    ä½¿ç”¨é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–çš„åˆ†éš”ç¬¦ã€‚
    
    Args:
        documents: Document åˆ—è¡¨
        
    Returns:
        åˆ†å—åçš„ Document åˆ—è¡¨
    """
    # ä¸­æ–‡å‹å¥½çš„åˆ†éš”ç¬¦ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    separators = [
        "\n\n",      # æ®µè½
        "\n",        # æ¢è¡Œ
        "ã€‚",        # å¥å·
        "ï¼",        # æ„Ÿå¹å·
        "ï¼Ÿ",        # é—®å·
        "ï¼›",        # åˆ†å·
        "ï¼Œ",        # é€—å·
        " ",         # ç©ºæ ¼
        ""           # å­—ç¬¦çº§åˆ«ï¼ˆæœ€åæ‰‹æ®µï¼‰
    ]
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=separators,
        length_function=len,
    )
    
    chunks = splitter.split_documents(documents)
    logger.info(f"âœ‚ï¸ åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªæ–‡æ¡£å—")
    return chunks


def build_index(
    pdf_path: Optional[Path] = None,
    persist_directory: Optional[Path] = None,
    force_rebuild: bool = False
) -> "Chroma":
    """æ„å»ºå‘é‡ç´¢å¼•.
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
        persist_directory: æŒä¹…åŒ–ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
        
    Returns:
        ChromaDB å‘é‡æ•°æ®åº“å®ä¾‹
    """
    from langchain_chroma import Chroma
    
    pdf_path = pdf_path or PDF_PATH
    persist_directory = persist_directory or CHROMA_PERSIST_DIR
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç´¢å¼•
    if persist_directory.exists() and not force_rebuild:
        logger.info(f"ğŸ“‚ å‘ç°å·²æœ‰ç´¢å¼•: {persist_directory}")
        logger.info("å¦‚éœ€é‡å»ºï¼Œè¯·ä½¿ç”¨ force_rebuild=True")
        return Chroma(
            persist_directory=str(persist_directory),
            embedding_function=get_embeddings(),
            collection_name=COLLECTION_NAME
        )
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    persist_directory.mkdir(parents=True, exist_ok=True)
    
    logger.info("ğŸš€ å¼€å§‹æ„å»ºç´¢å¼•...")
    
    # 1. åŠ è½½ PDF
    documents = load_pdf(pdf_path)
    
    # 2. é¢„å¤„ç†
    processed_docs = preprocess_documents(documents)
    
    # 3. åˆ†å—
    chunks = split_documents(processed_docs)
    
    # 4. è·å– Embedding æ¨¡å‹
    embeddings = get_embeddings()
    
    # 5. åˆ›å»ºå‘é‡æ•°æ®åº“
    logger.info("ğŸ”„ æ­£åœ¨å‘é‡åŒ–å¹¶å­˜å‚¨ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
    
    vectordb = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=str(persist_directory),
        collection_name=COLLECTION_NAME
    )
    
    logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ!")
    logger.info(f"   - æ–‡æ¡£å—æ•°é‡: {len(chunks)}")
    logger.info(f"   - å­˜å‚¨ä½ç½®: {persist_directory}")
    
    return vectordb


def get_index_stats(persist_directory: Optional[Path] = None) -> dict:
    """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯.
    
    Args:
        persist_directory: æŒä¹…åŒ–ç›®å½•
        
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    from langchain_chroma import Chroma
    
    persist_directory = persist_directory or CHROMA_PERSIST_DIR
    
    if not persist_directory.exists():
        return {"exists": False, "error": "ç´¢å¼•ä¸å­˜åœ¨"}
    
    try:
        vectordb = Chroma(
            persist_directory=str(persist_directory),
            embedding_function=get_embeddings(),
            collection_name=COLLECTION_NAME
        )
        
        # è·å–é›†åˆä¿¡æ¯
        collection = vectordb._collection
        count = collection.count()
        
        return {
            "exists": True,
            "document_count": count,
            "collection_name": COLLECTION_NAME,
            "persist_directory": str(persist_directory)
        }
    except Exception as e:
        return {"exists": False, "error": str(e)}


# ============================================================
# CLI å…¥å£
# ============================================================

def main():
    """å‘½ä»¤è¡Œå…¥å£."""
    import argparse
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s"
    )
    
    parser = argparse.ArgumentParser(
        description="DND è§„åˆ™ä¹¦ PDF ç´¢å¼•æ„å»ºå·¥å…·"
    )
    parser.add_argument(
        "--pdf",
        type=str,
        default=str(PDF_PATH),
        help=f"PDF æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {PDF_PATH})"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=str(CHROMA_PERSIST_DIR),
        help=f"ç´¢å¼•è¾“å‡ºç›®å½• (é»˜è®¤: {CHROMA_PERSIST_DIR})"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡å»ºç´¢å¼•ï¼ˆè¦†ç›–å·²æœ‰ç´¢å¼•ï¼‰"
    )
    parser.add_argument(
        "--stats",
        action="store_true",
        help="ä»…æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"
    )
    
    args = parser.parse_args()
    
    if args.stats:
        stats = get_index_stats(Path(args.output))
        print("\nğŸ“Š ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            print(f"   {key}: {value}")
        return
    
    # æ„å»ºç´¢å¼•
    build_index(
        pdf_path=Path(args.pdf),
        persist_directory=Path(args.output),
        force_rebuild=args.force
    )


if __name__ == "__main__":
    main()

